{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Requests Library\n",
    "\n",
    "Now that we know how to use BeautifulSoup to get data from HTML files, let's see how we can scrape data from a real website. Unfortunately, Beautifulsoup can't access websites directly. Therefore, in order to access websites, we will use Python's `requests` library. The `requests` library allows us to send web requests and get a website's HTML data. Once the `requests` library gets us the HTML data, we can use Beautifulsoup, just as we did before, to extract the data we want. So let's see an example.\n",
    "\n",
    "In the code below we will use the `requests` library and BeautifulSoup to get Tesla's `production and sales by quarter` data from a `html table` the following Wikipedia [webpage](https://en.wikipedia.org/wiki/Tesla,_Inc.). This table corresponds to Tesla's production and sales figures since Q1 2013. We will start by importing the `requests` library by using:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "```\n",
    "\n",
    "We will then use the `requests.get(website)` function to get the source code from our `wikipage`. The `requests.get()` function returns a `Response` object that we will save in the variable `r`. We can get the HTML data we need from this object by using the `.text` method, as shown below. Finally, we'll convert and display the extracted html table into Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# NLTK VADER for sentiment analysis\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "finwiz_url = 'https://finviz.com/quote.ashx?t='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\HP-Z440\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_tables = {}\n",
    "tickers = ['FGPHF']\n",
    "\n",
    "for ticker in tickers:\n",
    "    url = finwiz_url + ticker\n",
    "    req = Request(url=url,headers={'user-agent': 'my-app/0.0.1'}) \n",
    "    try:\n",
    "        response = urlopen(req)    \n",
    "    except:\n",
    "        continue\n",
    "    # Read the contents of the file into 'html'\n",
    "    html = BeautifulSoup(response)\n",
    "    # Find 'news-table' in the Soup and load it into 'news_table'\n",
    "    news_table = html.find(id='news-table')\n",
    "    # Add the table to our dictionary\n",
    "    news_tables[ticker] = news_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'FGPHF'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f9e50d5467cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read one single day of headlines for 'AMZN'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mamzn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnews_tables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FGPHF'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Get all the table rows tagged in HTML with <tr> into 'amzn_tr'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mamzn_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mamzn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindAll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'FGPHF'"
     ]
    }
   ],
   "source": [
    "# Read one single day of headlines for 'AMZN' \n",
    "amzn = news_tables['FGPHF']\n",
    "# Get all the table rows tagged in HTML with <tr> into 'amzn_tr'\n",
    "amzn_tr = amzn.findAll('tr')\n",
    "\n",
    "for i, table_row in enumerate(amzn_tr):\n",
    "    # Read the text of the element 'a' into 'link_text'\n",
    "    a_text = table_row.a.text\n",
    "    # Read the text of the element 'td' into 'data_text'\n",
    "    td_text = table_row.td.text\n",
    "    # Print the contents of 'link_text' and 'data_text' \n",
    "    print(a_text)\n",
    "    print(td_text)\n",
    "    # Exit after printing 4 rows of data\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_news = []\n",
    "\n",
    "# Iterate through the news\n",
    "for file_name, news_table in news_tables.items():\n",
    "    # Iterate through all tr tags in 'news_table'\n",
    "    for x in news_table.findAll('tr'):\n",
    "        # read the text from each tr tag into text\n",
    "        # get text from a only\n",
    "        text = x.a.get_text() \n",
    "        # splite text in the td tag into a list \n",
    "        date_scrape = x.td.text.split()\n",
    "        # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "            \n",
    "        # else load 'date' as the 1st element and 'time' as the second    \n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "        # Extract the ticker from the file name, get the string up to the 1st '_'  \n",
    "        ticker = file_name.split('_')[0]\n",
    "        \n",
    "        # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "        parsed_news.append([ticker, date, time, text])\n",
    "        \n",
    "parsed_news[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the sentiment intensity analyzer\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Set column names\n",
    "columns = ['ticker', 'date', 'time', 'headline']\n",
    "\n",
    "# Convert the parsed_news list into a DataFrame called 'parsed_and_scored_news'\n",
    "parsed_and_scored_news = pd.DataFrame(parsed_news, columns=columns)\n",
    "\n",
    "# Iterate through the headlines and get the polarity scores using vader\n",
    "scores = parsed_and_scored_news['headline'].apply(vader.polarity_scores).tolist()\n",
    "\n",
    "# Convert the 'scores' list of dicts into a DataFrame\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "# Join the DataFrames of the news and the list of dicts\n",
    "parsed_and_scored_news = parsed_and_scored_news.join(scores_df, rsuffix='_right')\n",
    "\n",
    "# Convert the date column from string to datetime\n",
    "parsed_and_scored_news['date'] = pd.to_datetime(parsed_and_scored_news.date).dt.date\n",
    "\n",
    "parsed_and_scored_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 10]\n",
    "\n",
    "# Group by date and ticker columns from scored_news and calculate the mean\n",
    "mean_scores = parsed_and_scored_news.groupby(['ticker','date']).mean()\n",
    "\n",
    "# Unstack the column ticker\n",
    "mean_scores = mean_scores.unstack()\n",
    "\n",
    "# Get the cross-section of compound in the 'columns' axis\n",
    "mean_scores = mean_scores.xs('compound', axis=\"columns\").transpose()\n",
    "\n",
    "# Plot a bar chart with pandas\n",
    "mean_scores.plot(kind = 'bar')\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "147px",
    "width": "322px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
